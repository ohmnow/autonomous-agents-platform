# 2025-12-12 (GPT-5.2) — Bugs / Issues / Risks Noted in Current Codebase

This is a “notes” doc: some items are definite bugs; others are reliability/security/UX risks worth addressing.

## Build execution + artifact pipeline
- **E2B key detection likely misfires**: `build-runner.ts` treats any `E2B_API_KEY` value other than the literal `'your_e2b_api_key_here'` as “real”. Your `env.example` uses a different placeholder (`e2b_your-api-key-here`), so local dev can accidentally try “real” mode with an invalid key and fail.
- **Build runner doesn’t populate `Build.sandboxId` or `Build.startedAt`**: the Prisma model and DB helpers support them, but `startBuildInBackground` never calls `startBuild(...)` nor `updateBuild(...startedAt/sandboxId...)`. This makes it harder to correlate builds ↔ sandboxes and to compute accurate elapsed time.
- **Async flushing is not awaited**:
  - `startLogFlusher()` calls `flushLogBuffer()` / `flushEventBuffer()` on an interval without awaiting; final flush on stop also isn’t awaited. On process shutdown or fast completion, you can lose the tail of logs/events.
  - If `createMany` fails and logs are re-buffered, there’s no backoff; it can churn.
- **In-memory state is per-process**: `activeBuildLogs/events/subscribers/sandboxes` live in memory. In multi-instance deployments (or even after a restart), SSE “live streaming” becomes best-effort and relies on DB polling.

## SSE streaming + polling correctness
- **Polling uses string comparison for “last ID”**:
  - In `/api/builds/[id]/stream`, polling compares `logId > lastLogId` and `eventId > lastEventId`. Prisma `id` values (cuid) are not ordered lexicographically by creation time, and in-memory IDs are random-ish (`log_${Date.now()}_${Math.random()...}`), so this can:
    - resend duplicates, or
    - skip some records, depending on ordering.
  - If you want polling, it should use `createdAt > lastSeenTime` (or a DB cursor) rather than string comparison.
- **Polling reloads up to 1000 historical records each cycle**: `loadHistoricalLogs/events` always loads from the beginning (up to 1000) and then filters client-side. This can become expensive quickly.

## Sandbox provider interface gaps vs roadmap
- **No “port exposure / preview URL” primitive**: `Sandbox` interface has exec/read/write/download/destroy, but no `exposePort()` / `getUrl()` / `forwardPort()` method. That’s a blocker for “view build in sandbox”.
- **E2B wrapper uses `@e2b/code-interpreter`**: great for command execution and file I/O, but (as currently wrapped) there’s no visible capability to publish a running web server to a URL.

## Artifact packaging choices may conflict with “preview”
- `downloadDir()` excludes: `.next`, `dist`, `build`, `.cache`, etc. That’s good for small zips, but it may remove exactly the directories you’d need for:
  - serving a production build from artifacts, or
  - doing post-build preview without re-building.
- `tar → zip` conversion loads whole archives in memory. Large workspaces can cause **high memory usage** on the Next server.

## Env/config consistency issues
- **`apps/web/README.md` is the default Next template** (not describing your platform or its commands). This is small, but it confuses contributors.
- **DATABASE_URL mismatch risk**: `docker-compose.yml` uses `agents/agents_password/agents_platform`, while `env.example` uses `postgres:password@.../autonomous_agents`. If devs follow both, they’ll hit connection errors unless they reconcile values.

## Auth + rate limiting caveats
- **Rate limiting is in-memory** (`apps/web/src/lib/rate-limit.ts`): fine for single instance, but ineffective in multi-instance deployments (and can be reset by restarts).
- **Preview/security concerns (future)**: once you add “view sandbox”, you’ll need to treat that surface like an untrusted deployment:
  - authentication/authorization gating,
  - URL leakage prevention,
  - SSRF/cross-tenant access protection,
  - strong sandbox networking policy and timeouts.


